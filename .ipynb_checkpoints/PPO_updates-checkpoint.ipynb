{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import stuff and make environments\n",
    "\n",
    "import gym\n",
    "import inventory\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "env = make_vec_env('inventory-v1', n_envs=8)\n",
    "test_env = gym.make('inventory-v1')\n",
    "\n",
    "\n",
    "#best constant order policy evaluation\n",
    "h_ = 1\n",
    "p_ = 99\n",
    "lamb_ = 1\n",
    "tau_ph = (h_/(2*p_+h_))**(1/2)\n",
    "r = (1/lamb_)*(1-tau_ph) #value for constant order policy\n",
    "r_performance = (1/lamb_)*((h_*(2*p_ + h_))**(1/2)-h_)\n",
    "\n",
    "print(r, r_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train a policy and test its performance\n",
    "\n",
    "n_timesteps = 100000      #how many learning steps?\n",
    "n_episodes = 100          #how many episodes to use when evaluating policy?\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=1, n_steps=512, gamma = 1)\n",
    "#default = model.policy.state_dict() # get model parameters\n",
    "#mines = default.copy() # make a copy to edit\n",
    "#mines[\"log_std\"] = torch.tensor([-3], device=mines[\"log_std\"].device) # set initial log std of actions taken by policy\n",
    "#mines[\"action_net.weight\"][0] = torch.zeros(len(mines[\"action_net.weight\"][0]), device=mines[\"action_net.weight\"].device) # set weights of last layer of policy network to 0 so we can implement approximately constant order policy\n",
    "#mines[\"action_net.bias\"] = torch.tensor([.929], device=mines[\"action_net.bias\"].device) # approximately setting mean action to optimal constant order amount\n",
    "#model.policy.load_state_dict(mines) # initialize with our custom parameters \n",
    "    \n",
    "model.learn(n_timesteps)\n",
    "res_mean, res_std = evaluate_policy(model, test_env, n_eval_episodes=n_episodes)\n",
    "print(-res_mean,'+/-',1.96*res_std/np.sqrt(n_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train several policies of different lengths and plot their performances\n",
    "# (can be parallelized)\n",
    "\n",
    "n_episodes = 20      #how many episodes to use when evaluating policy?\n",
    "n_learning = 5       #how many policies to train?\n",
    "n_timesteps = 10000  #difference in number of learning steps between successive polices?\n",
    "\n",
    "model_performance = np.zeros(n_learning)\n",
    "learning_steps = range(n_timesteps, (n_learning+1)*n_timesteps, n_timesteps)\n",
    "\n",
    "for i in range(0, n_learning):\n",
    "    model = PPO(MlpPolicy, env, n_steps=512, gamma = 1)\n",
    "    model.learn((i+1)*n_timesteps)\n",
    "    res_mean, res_std = evaluate_policy(model, test_env, n_eval_episodes=n_episodes)\n",
    "    print((i+1)*n_timesteps)\n",
    "    print(-res_mean,'+/-',1.96*res_std/np.sqrt(n_episodes))\n",
    "    model_performance[i] = -res_mean\n",
    "\n",
    "plt.plot(learning_steps, model_performance)\n",
    "plt.xlabel(\"Number of Learning Steps\")\n",
    "plt.ylabel(\"Approximate Long Run Average Cost\")\n",
    "plt.title(\"Number of Learning Steps vs Policy Performance\")\n",
    "plt.axhline(y=r_performance, color = 'r', linestyle = \"--\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train a single policy, evaluate performance at several points, save best policy as you go\n",
    "# (cannot be parallelized)\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "n_episodes = 100        #how many episodes to use when evaluating policy?\n",
    "n_timesteps = 50000     #how many learning steps?\n",
    "eval_frequency = 10000  #how many learning steps between policy evaluations?\n",
    "#cant figure out how to increase episode length when evaluating?\n",
    "\n",
    "eval_callback = EvalCallback(test_env, best_model_save_path = './logs/',\n",
    "                             log_path = './logs/', eval_freq = eval_frequency,\n",
    "                             render = False, n_eval_episodes = n_episodes)\n",
    "\n",
    "model = PPO(MlpPolicy, test_env, verbose=1, n_steps=512, gamma = 1)\n",
    "model.learn(50000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
