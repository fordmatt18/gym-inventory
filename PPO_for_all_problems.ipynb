{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import inventory\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from inventory.envs.inventory_env import Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=400, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=500, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=600, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=700, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=800, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=900, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1000, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1100, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1200, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1300, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1400, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1600, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1700, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-0.22 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1900, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=-0.23 +/- 0.01\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=100, episode_reward=-0.73 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200, episode_reward=-0.73 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=300, episode_reward=-0.73 +/- 0.03\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400, episode_reward=-0.73 +/- 0.03\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=500, episode_reward=-0.72 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600, episode_reward=-0.75 +/- 0.03\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=700, episode_reward=-0.73 +/- 0.03\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=800, episode_reward=-0.73 +/- 0.02\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=900, episode_reward=-0.72 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-0.76 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1100, episode_reward=-0.73 +/- 0.03\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1200, episode_reward=-0.72 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1300, episode_reward=-0.71 +/- 0.02\n",
      "Episode length: 500.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400, episode_reward=-0.73 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1500, episode_reward=-0.72 +/- 0.02\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1600, episode_reward=-0.73 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1700, episode_reward=-0.74 +/- 0.03\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1800, episode_reward=-0.73 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=1900, episode_reward=-0.75 +/- 0.03\n",
      "Episode length: 500.00 +/- 0.00\n",
      "Eval num_timesteps=2000, episode_reward=-0.72 +/- 0.04\n",
      "Episode length: 500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "# train and save models for each set of problem parameters\n",
    "\n",
    "\n",
    "# these are parameters I used just to test code, please run parameters that are not commented out\n",
    "\n",
    "# train_length = 500 # episode length when training\n",
    "# n_train_episodes = 8 # number of episodes to simulate in parellel during training\n",
    "# eval_length = 500 # episode length when evaluating\n",
    "# training_steps = 1000 # how many total time steps to use for training\n",
    "# n_eval_episodes = 10          #how many episodes to use when evaluating policy \n",
    "# eval_frequency = 100  # how many time steps between checkpoints in training\n",
    "# problems = [[1/4,1],[1,1]]\n",
    "\n",
    "\n",
    "train_length = 5000 # episode length when training\n",
    "n_train_episodes = 8 # number of episodes to simulate in parellel during training\n",
    "eval_length = 50000 # episode length when evaluating\n",
    "training_steps = 5000000 # how many total time steps to use for training\n",
    "n_eval_episodes = 10          #how many episodes to use when evaluating policy \n",
    "eval_frequency = 10000  # how many time steps between checkpoints in training\n",
    "problems = [[1/4,1],[1,1],[4,1],[9,1],[39,1],[99,1],\n",
    "            [1/4,4],[1,4],[4,4],[9,4],[39,4],[99,4],\n",
    "            [1/4,10],[1,10],[4,10],[9,10],[39,10],[99,10],\n",
    "            [1/4,20],[1,20],[4,20],[9,20],[39,20],[99,20],\n",
    "            [1/4,30],[1,30],[4,30],[9,30],[39,30],[99,30],\n",
    "            [1/4,50],[1,50],[4,50],[9,50],[39,50],[99,50],\n",
    "            [1/4,70],[1,70],[4,70],[9,70],[39,70],[99,70],\n",
    "            [1/4,100],[1,100],[4,100],[9,100],[39,100],[99,100]]\n",
    "\n",
    "for p, L in problems:\n",
    "    train_parameters = {\"p\": p, \"L\": L, \"length\": train_length}\n",
    "    eval_parameters = train_parameters.copy()\n",
    "    eval_parameters[\"length\"] = eval_length\n",
    "    train_env = make_vec_env(Inventory, n_envs=n_train_episodes, env_kwargs = train_parameters)\n",
    "    eval_env = make_vec_env(Inventory, n_envs=1, env_kwargs = eval_parameters)\n",
    "    save_path =  './logs/' + str(p) + \"_\" + str(L)\n",
    "    eval_callback = EvalCallback(eval_env, best_model_save_path = save_path,\n",
    "                             log_path = save_path, eval_freq = eval_frequency,\n",
    "                             render = False, n_eval_episodes = n_eval_episodes)\n",
    "\n",
    "    model = PPO(MlpPolicy, eval_env, verbose=False, gamma = 1)\n",
    "    default = model.policy.state_dict() # get model parameters\n",
    "    mine = default.copy() # make a copy to edit\n",
    "    #mine[\"log_std\"] = torch.tensor([-1], device=mine[\"log_std\"].device) # set initial log std of actions taken by policy\n",
    "    mine[\"action_net.weight\"][0] = torch.zeros(len(mine[\"action_net.weight\"][0]), device=mine[\"action_net.weight\"].device) # set weights of last layer of policy network to 0 so we can implement approximately constant order policy\n",
    "    mine[\"action_net.bias\"] = torch.tensor([train_env.envs[0].env.opt_const], device=mine[\"action_net.bias\"].device) # approximately setting mean action to optimal constant order amount\n",
    "    model.policy.load_state_dict(mine) # initialize with our custom parameters \n",
    "    model.learn(training_steps, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform evaluation of the best models selected in the previous step for each problem and save results\n",
    "\n",
    "# evaluate \n",
    "ratios = []\n",
    "ratio_95_conf_ints = []\n",
    "for p, L in problems:\n",
    "    const_order_val = (2*p + 1)**(1/2)-1\n",
    "    load_path =  './logs/' + str(p) + \"_\" + str(L)\n",
    "    model = PPO.load(load_path + \"/best_model.zip\")\n",
    "    env = make_vec_env(Inventory, n_envs=1, env_kwargs = eval_parameters)\n",
    "    rew_mean, rew_std = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes)\n",
    "    rew_mean = -rew_mean\n",
    "    ratios.append(const_order_val/(rew_mean))\n",
    "    half_width = 1.96*rew_std/np.sqrt(n_eval_episodes)\n",
    "    ratio_95_conf_ints.append([const_order_val/(rew_mean+half_width), const_order_val/(rew_mean-half_width)])\n",
    "\n",
    "# save\n",
    "with open('objs.pkl', 'wb') as f:\n",
    "    pickle.dump([problems, ratios, ratio_95_conf_ints], f)\n",
    "    f.close()\n",
    "\n",
    "# if we need to load those objects\n",
    "#with open('objs.pkl', 'rb') as f:\n",
    "#    problems, ratios, ratio_95_conf_ints = pickle.load(f)\n",
    "#    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2663916652432184, 1.0207682750072293]\n",
      "[[0.2575173236370867, 0.275899476881756], [0.9806028861757383, 1.064364531843326]]\n"
     ]
    }
   ],
   "source": [
    "# ratios are pretty bad because I didn't train for long at all\n",
    "print(ratios)\n",
    "print(ratio_95_conf_ints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
